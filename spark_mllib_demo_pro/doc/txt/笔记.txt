Spark机器学习算法库主要有两大类

从spark2.0开始，基于rdd api实现的算法库，进入到维护模式，spark官方建议你在使用spark机器学习框架的时候，建议你使用基于
DataFrame API实现的算法库Spark-ML

1，基于DataFrame API实现的算法库
   Spark-ML
   官方说明文档：http://spark.apache.org/docs/latest/ml-guide.html
   所在的包：org.apache.spark.ml


2，基于RDD API实现的算法库
   Spark-MLlib
   官方说明文档：http://spark.apache.org/docs/latest/mllib-guide.html
   所在的包：org.apache.spark.mllib
==============================================================================================================================
在spark ml或mllib中，主要提供了四大类我们常用的算法：

一，分类算法（Classification）
	分类算主要用于，预测两个类别中的一个或多个类别中的一个，结果是一个离散的值，它是监督学习算法

	1，二分类算法（Binary Classification）
	   a,支持向量机（linear SVMs）
	   b,逻辑回归算法（logistic regression）
	   c,决策树（decision trees）
	   d,朴素贝叶斯算法（naive Bayes）
	   e,随机森林算法（Random Forests）
	   f,梯度提升树算法（Gradient-Boosted Trees）


	   应用场景：
		预测人的性别：男  女
		预测广告是否点击：是  否

	2，多分类算法（Multiclass Classification）
		a,逻辑回归算法（logistic regression）
		b,决策树（decision trees）
		c,朴素贝叶斯算法（naive Bayes）
		d,随机森林算法（Random Forests）

		运用常用：
		预测图形类别：圆形，长方形，正方形...
		预测水果类别：香蕉，苹果，葡萄，梨...


二，回归算法（regression）
	主要作用：预测一个连续的值，结果是连续的值，也是监督学习算法
	应用场景：
		预测气温的趋势（4-15°）
		预测订单量（300-500单）

	a,线性回归（linear regression）
		进行参数优化后，会得到两个优化的线性回归算法
		L1正则化===》 Lasso regression
		L2正则化===》 ridge regression

	b,决策树（decision trees）
	c,随机森林算法（Random Forests）
	d,梯度提升树算法（Gradient-Boosted Trees）



集成Ensembles 学习算法（融合学习算法）:可以做分类也能用于进行回归问题预测
将过个算法集成起来进行预测，然后将结果进行合并
	a,随机森林算法（Random Forests）(可以处理二分类和多分类问题)
	   底层是由N课决策树构成

	b,梯度提升树算法（Gradient-Boosted Trees）（在spark中仅支持二分类）




三，聚类算法（Clustering）
	它是无监督学习算法（没有样本数据作为参考），典型聚类算法是
	k-means算法（K-均值算法）




四，推荐算法（Collaborative filtering）
	它的主要作用用于商品的一些推荐，比如：电商网站看了又看，买了有买
	或在头条上看新闻，会根据人的喜好进行推荐
	交替最小二乘法（alternating least squares (ALS)）：根据用户对商品的评价，来评判用户的喜好


五，频繁模式挖掘算法（Frequent pattern mining）
	也称之为关联规则算法，主要用于发现寻找事物之间的关联度，绝大多数用于电商推荐
	FP-growth：频繁模式增长挖掘算法
	association rules：关联规则算法
	PrefixSpan：前缀投影模式挖掘算法，频繁序列挖掘算法
==============================================================================================================================
推荐系统

1，通过探寻不同物品(产品 Item，Product)或不同用户(User)之间喜好的联系，用于预测用户所喜欢的商品

2，推荐系统应用案例：
	亚马逊推荐系统案例：
	产品：Comotomo 可么多么 硅胶替换乳头
		 推荐1：经常一起购买的商品
		 推荐2：浏览此商品的顾客也同时浏览
		 推荐3：购买此商品的顾客也同时购买
		 推荐4：看过此商品后顾客买的其它商品？

	京东推荐系统案例：
	产品：ThinkPad E15 15.6英寸窄边框笔记本电脑
		推荐1：人气配件，经常一起购买的商品
		推荐2：人气配件top5进行推荐
		推荐3：达人选购，经常购买同类产品的用户，他们的选择
		推荐4：看了又看(浏览此商品的顾客也同时浏览)
		推荐5：笔记本热销榜(同价位，同品牌)

	百度搜索引擎推荐案例：
	内容：天龙八部电影
	推荐方式：根据搜索内容的相似，推荐类似的电影
3，推荐系统会使用的数据：用户行为数据或用户购买数据

4，推荐系统的本质：寻找不同物品或不用用户之间的相似度

5，推荐系统和搜索引擎的区别：
	推荐系统：物品来主动找用户
	搜索引擎：用户主动找物品


6，推荐系统的组成部分
一个完整的推荐系统应该包含：产品、系统、消息中间件、搜索引擎 、NoSQL、分布式计算引擎、算法、效果评测  这些部分构成

7，推荐系统的核心组成部分
    产品：商品或产品(item product)
	系统：产生用户行为数据(用户浏览数据或购买数据)
	算法：基础算法(分类算法，回归算法，聚类算法) ，推荐算法

8，在整个推荐系统算法中，推荐算法是整个推荐系统中最核心、最关键的部分，很大程度上决定了推荐系统性能的优劣。
==============================================================================================================================
协同过滤推荐
	核心思想：观察所有用户（User）对所有产品（Product）的评价（Rating），来预测用户的喜好，进而给用户推荐商品

	功能：
		1，针对用户来说：推荐哪些商品给用户
		2，对这产品来说：将产品推荐给哪些用户


    用户对产品的评价：
		1，显示评价（能够直观的反应用户对该商品的喜好）
		   显示评价的数据包含：用户直接评分（五星好评），用户收藏次数

		2，隐式评价(需要经过一定的分析和处理才能反应用户对该商品的喜好程度)
		   隐式评价的数据包含：浏览商品的时间长度，浏览次数，.....

在机器学习算法中，ALS（交替最小二乘法（alternating））这个算法可以实现协同过滤推荐，Spark机器学习库也有具体的实现

机器学习在进行预测的一个具体流程
	数据===>机器学习算法====>模型===>预测


ALS交替最小二乘法推荐算法
	用户对产品的评价数据（Rating）===>ALS算法===>矩阵分解模型（MatrixFactorizationModel）===>预测出用户的喜好


ALS如何实现协同过滤思想
	在ALS算法中，用户User对产品Product的评价Rating数据会放在一个矩阵中

	矩阵中的每一行代表的是用户对所有商品的评价数据
	矩阵中的每一列代表的是商品被所有用户评价的数据


	矩阵：可以看出是一个二维表格，矩阵中的每一行可以看出是一个向量。即矩阵是由向量构成
		  稀疏矩阵：由稀疏向量构成
		  稠密矩阵：由稠密向量构成

	向量：其实就是一个数组
		  稀疏向量：值为0的占50%以上，大多数
		  稠密向量：值为非0的占50%以上，大多数

	由于用户对产品的评价是一个稀疏矩阵，不好评判用户是否对某个商品感兴趣，所以需要将稀疏矩阵X(n*m),进行分解
		1，用户因子矩阵

		2，产品因子矩阵

		评价稀疏矩阵		用户因子稠密矩阵				产品因子稠密矩阵
		X(n*m)   		=   	 U(n*k)             *           V(k*m)

	ALS矩阵分解得到的模型：矩阵分解模型（MatrixFactorizationModel）。一旦得到了这个模型，推荐求解就更容易


ALS算法如何使用
	数据==>Rating
	算法==>ALS
	模型==>MatrixFactorizationModel

	ALS算法需要的参数
		训练数据Rating
		特征值 K
		迭代次数 （交替次数）
==============================================================================================================================
在机器学习中

	数据===》算法====》模型 ====》预测或推荐

比较难的点是我们得到的模型是否是最佳模型，一个好的模型会影响预测或推荐效果

衍生出2个问题
	1，如何对模型进行评估
		不同的机器学习算法得到的模型，评估指标是不一样的，对于推荐算法来说
		常用的评估指标是 RMSE(均方根误差)，MSE(均方误差)，这两个误差结果肯
		定是越小越好，越小代表我们的推荐模型比较优秀，能够达到预期的推荐效果


	  对模型进行评估，Spark Mllib一般都提供了模型评估器，ALS交替最小二乘法推荐算法得到的矩阵分解模型是一个
	  回归模型，所以我们可以使用回归模型评估器对矩阵分解模型进行评估
	  import org.apache.spark.mllib.evaluation.RegressionMetrics
	  RegressionMetrics 回归模型评估器


	2，如何得到最佳模型
		1，调整训练参数

		2，调整数据
			针对推荐算法来说，最重要的是调整用户对产品的评价数据，将一些噪音数据进行降噪处理，尽量让数据合理不要太过于分散化
==============================================================================================================================
机器学习流程
			数据===》算法====》模型===》预测

对于分类算法来说：预处理===》提取特征工程==》算法

使用1-of-K(哑编码)将非数值类型的类别特征，转换成向量
		male			female
sex    Array(1,0)      Array(0,1)

思路：
	1，获取类别个数，以便定义类别对应向量的长度
	2，需要有一个Map("male"->0,"female"-->1)
	3，需要创建一个数组/向量  Array(0,0)
	4，male --> Array(1,0)    female-->Array(0,1)



对Age特征的处理
	思路：
		1，需要有一个Map("child"->0,"young"->1,"middle"->2,"old"->3)
		2, 在算子里面创建一个数组,数组的长度和上面map的size保存一致。Array[Double](0.0,0.0,0.0,0.0)
		3，获取乘客的年龄，进行判断，看看属于哪个区间，如果是child ，那么就将child在map中对应的角标0与数组Array对应的角标0的
			改为1.0得到一个数组Array(1.0,0.0,0.0,0.0)
==============================================================================================================================
多分类算法


鸢尾花
5.0,3.3,1.4,0.2,Iris-setosa
7.0,3.2,4.7,1.4,Iris-versicolor

	5.0,3.3 	鸢尾花花萼的长度和宽度
	1.4,0.2 	鸢尾花花瓣的长度和宽度
	Iris-setosa 鸢尾花类别


目标：通过鸢尾花的花萼和花瓣的长度和宽度，预测出鸢尾花的类别
	  鸢尾花的特征数据5.0,3.3,1.4,0.2  ====》多分类算法进行训练====》多分类算法模型=====》进行预测


思路：
	1，加载数据
	2，将鸢尾花的花萼和花瓣的长度和宽度作为特征封装到 向量Vector(Array(5.0,3.3,1.4,0.2))
	3，将鸢尾花的字符串类别转换成标签数据Map("Iris-setosa"->0,"Iris-versicolor"->1,"Iris-virginica"->2)
	4，最终要封装成

		5.0,3.3,1.4,0.2,Iris-setosa===》LabeledPoint(0,Vectors.dense(Array(5.0,3.3,1.4,0.2)))
==============================================================================================================================
Spark Mllib中支持两种回归
	1，线性回归（linear least squares） 线性最小二乘法回归
		在spark中线性回归有两种变形优化后的回归算法
			1.1 Lasso regression==》L1正则化
			1.2 ridge regression==》L2正则化
			正则化其实就是为了防止模型过拟合

	2，决策树回归
		2.1 决策树Decision Trees
		2.2 随机森林random forests
		2.2 梯度提升树gradient-boosted trees

在回归算法中，最重要的算法就是线性回归算法，它可以解决绝大多数回归问题，回归算法的目标其实就是找到一个函数，
这个函数能够很好的拟合样本数据，当新的数据过来时，这个函数（模型）能够较准确预测出结果（类别）

过拟合(OverFitting)：随着训练过程的进行，模型复杂度增加，在训练集上error渐渐减小，但是在测试集上的error却反而渐渐增大
	过拟合什么时候会出现：
		1，训练数据很少
		2，样本特征过多

模型泛化：训练获取的模型，可以很好的应用到新的数据(未出现在训练数据集中的数据)进行预测(值或类别)


如何防止模型过拟合，提高模型的泛化能力？
	1，选取较少的特征
	2，正则化（推荐的）


正则化作用：对模型进行正则化可以提高模型的泛化能力，防止模型过拟合

损失函数：在机器学习算法中，每个算法的核心其实就是找到一个损失函数
		  损失函数=1/2m * m*[(predict-actual)^2]
		  损失函数其实就是度量模型拟合样本的程度

当训练集数据很少，样本特征过多，那么会出现过拟合，也就意味着损失函数的值无限逼近与0，为了反正过拟合，会给损失函数加入一个
惩罚项，这个惩罚项就是正则化
	正则化项：
		L1正则化：正则化项对Q先求绝对值，然后在累加求和   Lasso regression==》L1正则化
		L2正则化：正则化项对Q先平方，再求和，最后平方根   ridge regression==》L2正则化

线性回归（linear least squares） 线性最小二乘法回归 没有进行正则化


一般来说，线性回归都伴随这随机梯度下降，目的是对模型进行优化



共享单车案例 字段说明
instant		序号
dteday		年月日时分秒日期格式
season		季节[1,2,3,4]
yr			年份[0,1]    0代表2011年  1代表2012年
mnth		月份[1,2,3,4,5,6,7,8,9,10,11,12]
hr			小时[0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,2,20,21,22,23]
holiday		是否是节假日[0,1]
weekday		一周第几天[0,1,2,3,4,5,6]
workingday	是否是工作日[0,1]
weathersit  天气状况[1,2,3,4]
temp		气温
atemp		体感温度
hum			湿度
windspeed	方向
casual		没有注册的用户租用自行车的数量
registered	注册的用户租用自行的数量
cnt			总的租用自行车的数量
==============================================================================================================================
分类，回归，它们都是监督学习方法，需要有样本数据作为参考，样本数据封装到LabeledPoint(标签，样本的特征向量)
LabeledPoint(label,Vector())

聚类算法是无监督学习方法，没有类别数据作为参考，只有特征

需要注意的是聚类算法不好评估模型的好坏
==============================================================================================================================
在推荐系统中，按推荐方式和方法的不同，主要有三种方式：
	1，基于内容推荐

	2，基于协同过滤推荐
		2.1 基于用户协同过滤
		2.2 基于物品的协同过滤
		2.3 基于模型的协同过滤
			ALS交替最小二乘法推荐算法，就是基于模型的协同过滤
			核心思想：将用户对产品的评价稀疏矩阵分解为用户因子矩阵和产品因子矩阵，去发现用户对产品的兴趣

	3，基于关联规则推荐
		使用关联规则算法，发现数据(购买数据、用户浏览数据)之间隐藏的关联性，经常用于电商网站中看了又看，买了有买

关联规则算法主要有： Apriori、FP Growth、PrefixSpan

在Spark机器学习库中，实现了FP Growth、PrefixSpan这两种关联规则算法


如何去评估数据项之间的关联度

举例：在一个超市中有10000个顾客购买了商品，其中有1000人购买了尿布，2000人购买了啤酒，同时在购买商品的顾客中有800人既购买了
尿布的同时也购买了啤酒

1，支持度(Support):这个值越大，表明关联度可能就越大
	表示在购买人群中，同时购买了X,Y物品的比例
	Support(啤酒&尿布)=800/10000=8%

	支持度用来衡量 数据项组合”出镜率“。一般我们会设置一个最小的支持对，用来过滤掉哪些关联度不高的数据项组合
	需要注意的是，支持度越高并不一定代表关联度就越高，但是支持度越小，那么关联度肯定小
	支持度是衡量数据项之间关联度的第一道门槛，是衡量"量"

2，置信度(Confidence)
    表示购买了X的商品的顾客，会购买Y商品的概率
	Confidence（啤酒==>尿布）=800/2000=0.4  购买啤酒的顾客有40%的概率会去购买尿布
	Confidence（尿布==>啤酒）=800/1000=0.8 	购买尿布的顾客有80%的概率会去购买啤酒
	置信度 是衡量数据项之间关联度的第二道门槛，是衡量"质"

	一般来说我们会在关联规则算法中设置一个最小的支持度和最小的置信度，满足这两个条件我们称之为强关联
	但是需要注意的是这种强关联并不一定是有效的强关联

3，提升度Lift：表示在发生x的条件下y发生的概率Confidence(x提升度是衡量数据项之间是否有效的强关联->y) 与 Y 发生的总体概率的比
	Lift(尿布==>啤酒)=Confidence(尿布==>啤酒)/P(y)= (800/1000)/(2000/10000)=4


	如果满足了最小支持度和最小置信度，那么数据项之间就是强关联，但是并不一定是有效的强关联
	当
	lift>1  有效的强关联
	lift<=1 无效的强关联

在Spark 机器学习关联规则算法中FP Growth、PrefixSpan主要考察的是支持度和置信度
Apriori算法还需要考察提升度
==============================================================================================================
文本分类

1	Hadoop is a big data framework
2	Spark is a big data framework
3	Spark is a big data framework

实现句子分类，首先对样本提取特征，样本是文本，文本如何提取特征？
Bag of Words 词袋模型
	思想：以文本中其中出现的单词的词频作为文本的特征向量
	核心：进行词频统计TF(Term Frequency)

	Array(Hadoop is a  big  data framework Spark)
1   Array(1      1  1  1    1       1         0 )
2   Array(0      1  1  1    1       1         1 )
3   Array(0      1  1  1    1       1         1 )

由于文本中有很多对文本分类没有任何意义的停用词，比如：is  a  the

如何解决上述停用词过多的问题？

TF-IDF(term frequency–inverse document frequency) 词频-逆文档频率
TF:单词在句子中出现的次数
DF：单词出现过的句子数/整个文档的句子总数 1/3

IDF：对DF进行颠倒 3/1

TF
	Array(Hadoop is a  big  data framework Spark)
1   Array(1      1  1  1    1       1         0 )
2   Array(0      1  1  1    1       1         1 )
2   Array(0      1  1  1    1       1         1 )

TF * IDF
	Array(Hadoop     is		 a 		 big 	 data 	  framework 	   Spark)
1   Array(1*3/1      1*3/3  1*3/3  	1*3/3    1*3/3       1*3/3         0*3/3 )
2   Array(0*3/1      1*3/3  1*3/3  	1*3/3    1*3/3       1*3/3         1*3/2 )
2   Array(0*3/1      1*3/3  1*3/3  	1*3/3    1*3/3       1*3/3         1*3/2 )


	Array(Hadoop     			is		 a 				 big 	 	data 			framework 		 	 Spark  )
1   Array(log(1*3/1)      log(1*3/3)  log(1*3/3)  	log(1*3/3)    log(1*3/3)       log(1*3/3)         log(0*3/3 ) )
2   Array(log(0*3/1)      log(1*3/3)  log(1*3/3)  	log(1*3/3)    log(1*3/3)       log(1*3/3)         log(1*3/2 ) )
2   Array(log(0*3/1)      log(1*3/3)  log(1*3/3)  	log(1*3/3)    log(1*3/3)       log(1*3/3)         log(1*3/2 ) )


在spark ML 机器学习库中有专门转换器和模型学习器帮助我们去实现

在Spark ML中只要是转换器，那么都会有一个transform方法，这个方法就是将一个dataframe转换成新的dataframe，有点类似于
spark core中rdd的transform算子

在进行词频统计TF时，一个单词其实就是代表一个特征

在Spark ML 中只要是Estimator，都需要先调用fit训练方法，训练出一个模型（转换器），然后再用这个模型（转换器）
的transform方法对dataframe进行转换
==============================================================================================================
简易文本数据分类案例

需要进行分词
需要进行TF进行词频统计
需要进行IDF进行特征提取
需要二分类算法(逻辑回归算法)
需要有管道
==============================================================================================================
在做机器学习过程中，获取最佳模型是最难的一步

如何获取最佳模型？

在Spark ML Pipeline 机器学习管道中，提供了交叉验证的方式
1，参数的交叉验证
2，数据集的交叉验证



















